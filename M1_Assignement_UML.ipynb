{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgprlXXFv1Fn"
      },
      "source": [
        "# Exercise for UML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT42Yxomv5I3"
      },
      "source": [
        "# Project Title - Spotify recommender system\n",
        "## About the Dataset\n",
        "These dataset contains 114000 song and metadata about the songs such as their popularity and genres. The exercise is divided in three part mainly EDA, PCA and Clustering and finally the recommender system.\n",
        "Try to write your own functions and know your keyboard shortcuts.\n",
        "You can work on GoogleCollab or work locally.\n",
        "\n",
        "The dataset: https://raw.githubusercontent.com/aaubs/ds-master/main/data/spotify_UML/spotify.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMfU_ASjwqnS"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjrDwVOnwtho"
      },
      "source": [
        "## Goals of Part 1\n",
        "    1. Clean up the dataset and check for duplicates\n",
        "    2. EDA\n",
        "    3. Plots\n",
        "## Relevant libraries for this part\n",
        "    1. Pandas\n",
        "    2. Numpy\n",
        "    3. Matplotlib\n",
        "    4. Seaborn\n",
        "    5. Pygwalker\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaH1kjv7w7TJ"
      },
      "source": [
        "## Exercises Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "19olq74-t-lm"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import altair as alt\n",
        "import pygwalker as pyg # this is installed in the terminal as well\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an_PzQ1YxBSb"
      },
      "outputs": [],
      "source": [
        "# Import the dataset. \n",
        "\n",
        "## Define the URL of the dataset\n",
        "url = \"https://raw.githubusercontent.com/aaubs/ds-master/main/data/spotify_UML/spotify.csv\"\n",
        "\n",
        "## Use Pandas to read the dataset into a DataFrame\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "## Display the first few rows of the dataset to understand its structure\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnMJGtoE8rcL"
      },
      "outputs": [],
      "source": [
        "# Understand the dataset. What columns are available?\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We can also get the columns by running this code\n",
        "columns = df.columns\n",
        "print(\"Columns available in the dataset:\")\n",
        "for column in columns:\n",
        "    print(column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To see the count number of distinct elements in specified axis:\n",
        "df.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRvySccp8xEs"
      },
      "outputs": [],
      "source": [
        "# Check for missing values. How would you handle them?\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values in the dataset:\")\n",
        "missing_values\n",
        "\n",
        "# There are null values (missing values) in three different columns 'artists', 'album_name' and 'track_name'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To show the columns which have missing values print this code\n",
        "missing_values = df.isna().any()\n",
        "missing_values[missing_values].index.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The way to handle them fill with 0\n",
        "df = df.fillna(0)\n",
        "df.isnull().sum() # Now there is no missing values left"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We have created box plots for some of the numeric columns to detect outliers \n",
        "plt.figure(figsize=(10, 6))\n",
        "df.boxplot(column=['popularity'])\n",
        "plt.title(\"Boxplots for Numeric Columns\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "df.boxplot(column=['duration_ms'])\n",
        "plt.title(\"Boxplots for Numeric Columns\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "df.boxplot(column=['danceability'])\n",
        "plt.title(\"Boxplots for Numeric Columns\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjbnJcyq84Z-"
      },
      "outputs": [],
      "source": [
        "# What are the distributions of song popularity, duration_ms, and danceability? Use appropriate visualizations.\n",
        "sns.displot(data=df,\n",
        "            x=\"popularity\",\n",
        "            kind=\"kde\")\n",
        "sns.displot(data=df,\n",
        "            x=\"duration_ms\",\n",
        "            kind=\"kde\")\n",
        "sns.displot(data=df,\n",
        "            x=\"danceability\",\n",
        "            kind=\"kde\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# How many unique genres are in the dataset? List the top 20. (Explain how you choose to list the top 20)\n",
        "# To calculate the number of different genres print the below\n",
        "unique_genres = df['track_genre'].nunique()\n",
        "print(\"Number of unique genres:\", unique_genres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For listing the genre in a list \n",
        "unique_values = df['track_genre'].unique()\n",
        "unique_values\n",
        "print(\"Unique Values in 'track_genre':\")\n",
        "for value in unique_values:\n",
        "    print(f\"- {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List the top 20. (Explain how you choose to list the top 20)\n",
        "top_20_genres = df['track_genre'].value_counts().head(20)\n",
        "print(\"Top 20 genres:\")\n",
        "print(top_20_genres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMkp76on-WMO"
      },
      "outputs": [],
      "source": [
        "# Visualize the number of songs by genre. Which are the most common genres?\n",
        "# There is 1000 song in each genre so all genres are equally represented in the dataset.\n",
        "genre_counts = df['track_genre'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "genre_counts.plot(kind='bar')\n",
        "\n",
        "plt.title('Number of Songs by Genre')\n",
        "plt.xlabel('Genre')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rank genres by the average popularity of their songs. Which genres tend to have more popular songs?\n",
        "popularity = df.groupby('track_genre')['popularity'].mean().sort_values(ascending=False)\n",
        "popularity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If we want to make a column in the df which shows the rank, we can follow the below steps.\n",
        "\n",
        "## Group by 'track_genre' and calculate the average popularity for each genre\n",
        "genre_popularity_rank = df.groupby('track_genre')['popularity'].mean().reset_index()\n",
        "\n",
        "## Rank genres by popularity in descending order\n",
        "genre_popularity_rank['popularity_rank'] = genre_popularity_rank['popularity'].rank(ascending=False, method='min')\n",
        "\n",
        "## Sort the DataFrame by popularity rank\n",
        "genre_popularity_rank = genre_popularity_rank.sort_values(by='popularity_rank')\n",
        "\n",
        "## Merge the popularity rank DataFrame with the original DataFrame\n",
        "df = df.merge(genre_popularity_rank[['track_genre', 'popularity_rank']], on='track_genre', how='left')\n",
        "\n",
        "## Display the updated DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore other characteristics (like danceability, energy, etc.) by genre. Are there any noticeable differences or trends?\n",
        "energy = df.groupby('track_genre')['energy'].mean().sort_values(ascending=False)\n",
        "energy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dance value is low for ex opera, sleep and so on which is expected\n",
        "dance = df.groupby('track_genre')['danceability'].mean().sort_values(ascending=False)\n",
        "dance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1 milliseconds (ms) is equal to 1.666667×10^-5 minutes (min). \n",
        "# Conversely, 1 minutes (min) is equal to 60000 milliseconds (ms).\n",
        "\n",
        "# Therefore, we have divided the milliseconds column with 6000 to get the duration in minutes\n",
        "df['duration_minutes'] = df['duration_ms'] / 60000\n",
        "\n",
        "# As below we can se a noticeable differences for the duration between the different genres\n",
        "duration_minutes = df.groupby('track_genre')['duration_minutes'].mean().sort_values(ascending=False)\n",
        "duration_minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ielqDlwV-vsw"
      },
      "outputs": [],
      "source": [
        "# Investigate the relationship between danceability and energy. Do songs that are more danceable tend to have more energy? Use a scatter plot.\n",
        "sns.relplot(data=df,\n",
        "            x=dance,\n",
        "            y=energy,\n",
        "            kind=\"scatter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We can also make a scatterplot wiht a linear regression line to see the relationship between danceability and energy\n",
        "\n",
        "# Set the sns.set to style=\"whitegrid\" to get gridlines\n",
        "# Set the Seaborn style to 'ggplot'\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a scatter plot with sns.relplot\n",
        "scatter_plot_dance_energy = sns.relplot(data=df, x=dance, y=energy, kind=\"scatter\")\n",
        "\n",
        "# Add a linear regression line using sns.regplot\n",
        "sns.regplot(data=df, x=dance, y=energy, scatter=False, ax=scatter_plot_dance_energy.ax)\n",
        "\n",
        "# Optional: Customize the plot\n",
        "plt.title(\"Scatter Plot with Linear Regression Line\")\n",
        "plt.xlabel(\"Danceability\")\n",
        "plt.ylabel(\"Energy\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# How does song popularity relate to other characteristics like danceability, loudness, or tempo?\n",
        "sns.relplot(data=df,\n",
        "            x=dance,\n",
        "            y=popularity,\n",
        "            kind=\"scatter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatterplot wiht a linear regression line \n",
        "scatter_plot_dance_popularity = sns.relplot(data=df, x=dance, y=popularity, kind=\"scatter\")\n",
        "sns.regplot(data=df, x=dance, y=popularity, scatter=False, ax=scatter_plot_dance_popularity.ax)\n",
        "\n",
        "# Optional: Customize the plot\n",
        "plt.title(\"Scatter Plot with Linear Regression Line\")\n",
        "plt.xlabel(\"Danceability\")\n",
        "plt.ylabel(\"Popularity\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FxRvL0u-99L"
      },
      "outputs": [],
      "source": [
        "# How do explicit songs compare to non-explicit ones in terms of popularity or other characteristics?\n",
        "# First we group data into explicit and non-explicit songs\n",
        "\n",
        "explicit_songs = df[df['explicit'] == True]  # Select rows where 'explicit' is True\n",
        "non_explicit_songs = df[df['explicit'] == False]  # Select rows where 'explicit' is False\n",
        "\n",
        "# Compare popularity using summary statistics\n",
        "explicit_popularity_mean = explicit_songs['popularity'].mean()\n",
        "non_explicit_popularity_mean = non_explicit_songs['popularity'].mean()\n",
        "\n",
        "# Print the mean popularity for explicit and non-explicit songs\n",
        "print(f\"Mean Popularity for Explicit Songs: {explicit_popularity_mean}\")\n",
        "print(f\"Mean Popularity for Non-Explicit Songs: {non_explicit_popularity_mean}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Are there any trends related to tempo or time_signature?\n",
        "\n",
        "# As we can see from be bleow one the sweetspot looks like to se at tempo 110-130.\n",
        "tempo = df.groupby('track_genre')['tempo'].mean().sort_values(ascending=False)\n",
        "popularity = df.groupby('track_genre')['popularity'].mean().sort_values(ascending=False)\n",
        "sns.scatterplot(data=df, x=tempo, y=popularity)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot for time_signature\n",
        "time_signature = df.groupby('track_genre')['time_signature'].mean().sort_values(ascending=False)\n",
        "popularity = df.groupby('track_genre')['popularity'].mean().sort_values(ascending=False)\n",
        "sns.scatterplot(data=df, x=time_signature, y=popularity)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7D2uPGCM1oR"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt2GQRMLM3To"
      },
      "source": [
        "## Goals of Part 2\n",
        "    1. Pre-processing for PCA (encoding & scaling)\n",
        "    2. PCA and explanations of results\n",
        "    3. Clustering\n",
        "## Relevant libraries for this part\n",
        "    1. StandardScaler\n",
        "    2. PCA\n",
        "    3. KMeans\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing StandardScaler from scikit-learn (sklearn)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Importing PCA (Principal Component Analysis) from scikit-learn (sklearn)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Importing KMeans clustering algorithm from scikit-learn (sklearn)\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Importing LabelEncoder from scikit-learn (sklearn)\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect Data Types: \n",
        "# We have to confirm the data types within the columns. \n",
        "\n",
        "# We can do this by checking the unique data types present in the column\n",
        "for column in df.columns:\n",
        "    data_type = df[column].apply(type).unique()\n",
        "    print(f\"Column '{column}' data types: {data_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# As we can see from the above one there are colomns with more more than one data type.\n",
        "# To check for columns that have more than one data type we can also use the below code\n",
        "for column in df.columns:\n",
        "    unique_data_types = df[column].apply(type).unique()\n",
        "    if len(unique_data_types) > 1:\n",
        "        print(f\"Column '{column}' has multiple data types: {unique_data_types}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert all entries in 'track_name' and 'album_name' to strings\n",
        "df['artists'] = df['artists'].astype(str)\n",
        "df['track_name'] = df['track_name'].astype(str)\n",
        "df['album_name'] = df['album_name'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Encode all categorical columns (dtype as 'object') in the DataFrame\n",
        "for column in df.select_dtypes(include=['object']):\n",
        "    df[column + '_id'] = encoder.fit_transform(df[column])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "id_columns = [col for col in df.columns if '_id' in col]\n",
        "df[id_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We select the below features from the dataframe\n",
        "selected_columns = ['Unnamed: 0', 'danceability',  'energy', 'popularity',  'duration_ms', 'track_name_id',  'track_genre_id', 'track_id_id',  'artists_id',  'album_name_id', 'liveness',  'valence',  'tempo',  'time_signature']\n",
        "\n",
        "# Create a new DataFrame with the selected columns\n",
        "data_to_cluster = df[selected_columns]\n",
        "\n",
        "# Display the new DataFrame\n",
        "data_to_cluster.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle any missing or categorical data.\n",
        "# Standardize the dataset since PCA is sensitive to the magnitude of the data.\n",
        "scaler = StandardScaler()\n",
        "data_to_cluster_scaled = scaler.fit_transform(data_to_cluster)\n",
        "data_to_cluster_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVNhDbUYM1QA"
      },
      "outputs": [],
      "source": [
        "# Conduct a PCA on the song characteristics.\n",
        "# Create a PCA instance with the desired number of components\n",
        "\n",
        "# Choose the number of components you want to keep\n",
        "n_components = 5  \n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "# Fit the PCA model to the data and transform the data to the principal components\n",
        "X_pca = pca.fit_transform(data_to_cluster_scaled)\n",
        "\n",
        "# Visualize the explained variance for each principal component.\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Explained Variance Ratios:\", explained_variance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce the dataset's dimensions based on the PCA results and visualize the data in the reduced dimension space.\n",
        "plt.scatter(x=X_pca[:, 0], y=X_pca[:, 1], alpha=0.6, color='blue')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Result')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqNfn7F1VeKg"
      },
      "outputs": [],
      "source": [
        "# Choose a clustering algorithm (e.g., KMeans, DBSCAN, or Hierarchical).\n",
        "\n",
        "## We're using the Kmeans because we have a lot of datapoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine the optimal number of clusters (if needed, like in KMeans). explain how you get to that number of clusters\n",
        "\n",
        "# Initialize variables\n",
        "inertia_values = []\n",
        "k_range = range(1, 11)  # We will check for up to 10 clusters\n",
        "\n",
        "# Run K-means with different k values and store the inertia (sum of squared distances)\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_pca)\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow method graph (sum of squared distances for each 'k')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, inertia_values, marker='o')\n",
        "plt.title('Elbow Method for Optimal Number of Clusters')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k=4\n",
        "centroids = X_pca[np.random.choice(X_pca.shape[0], k, replace=False)]\n",
        "\n",
        "# Plot observations\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], alpha=0.6, color='blue')\n",
        "\n",
        "# Plot centroids\n",
        "sns.scatterplot(x=centroids[:, 0], y=centroids[:, 1], color='red', s=100)\n",
        "\n",
        "plt.title('PCA Reduced Data and Initial Centroids')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def k_means_simple(X_pca, k, max_iters=100):\n",
        "    # 1. Initialize the k cluster centroids\n",
        "    centroids = X_pca[np.random.choice(X_pca.shape[0], k, replace=False)]\n",
        "\n",
        "    for _ in range(max_iters):\n",
        "        # 2. Assign each data point to the closest centroid\n",
        "        distances = np.linalg.norm(X_pca - centroids[:, np.newaxis], axis=2)\n",
        "        labels = np.argmin(distances, axis=0)\n",
        "\n",
        "        # 3. Recompute the centroids\n",
        "        new_centroids = np.array([X_pca[labels == i].mean(axis=0) for i in range(k)])\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.all(centroids == new_centroids):\n",
        "            break\n",
        "\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return labels, centroids\n",
        "\n",
        "# Print the centroids\n",
        "labels, final_centroids = k_means_simple(X_pca, 4)\n",
        "print(\"Cluster centroids:\\n\", final_centroids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cluster the songs based on the reduced dimensions from PCA.\n",
        "# Plot observations after 100st interation\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], alpha=0.6, color='blue')\n",
        "\n",
        "# Plot centroids\n",
        "sns.scatterplot(x=final_centroids[:, 0], y=final_centroids[:, 1], color='red', s=100)\n",
        "\n",
        "plt.title('PCA Reduced Data and last interation Centroids')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the clusters and interpret any patterns. Write your interpretations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Assign each data point to the closest centroid\n",
        "distances = np.linalg.norm(X_pca - centroids[:, np.newaxis], axis=2)\n",
        "labels = np.argmin(distances, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Recompute the centroids\n",
        "new_centroids = np.array([X_pca[labels == i].mean(axis=0) for i in range(k)])\n",
        "new_centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot observations after 1st interation\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], alpha=0.6, color='blue')\n",
        "\n",
        "# Plot centroids\n",
        "sns.scatterplot(x=new_centroids[:, 0], y=new_centroids[:, 1], color='red', s=100)\n",
        "\n",
        "plt.title('PCA Reduced Data and 1st interation Centroids')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clusterer = KMeans(n_clusters=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clusterer.fit(data_to_cluster_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we can then copy the cluster-numbers into the original file and start exploring\n",
        "df['cluster'] = clusterer.labels_\n",
        "df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,2))\n",
        "sns.heatmap(pd.DataFrame(pca.components_, columns=data_to_cluster.columns), annot=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot observations after 100st interation\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], alpha=0.6, color='blue')\n",
        "\n",
        "# Plot centroids\n",
        "sns.scatterplot(x=final_centroids[:, 0], y=final_centroids[:, 1], color='red', s=100)\n",
        "\n",
        "plt.title('PCA Reduced Data and last interation Centroids')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMaEqiQeO_nm"
      },
      "source": [
        "# Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWtaD1EkPFPb"
      },
      "source": [
        "## Goals of Part 3\n",
        "    1. Vectorization   \n",
        "    2. Cosine similarities\n",
        "    3. Build and test recommender\n",
        "    Objective: Develop a basic music recommender system that suggests songs based on textual data and put it in a small grad.io app\n",
        "## Relevant libraries for this part\n",
        "    1. linear_kernel\n",
        "    2. TfidfVectorizer\n",
        "    3. grad.io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAMIpSaTYOW6"
      },
      "source": [
        "Build the Recommender:\n",
        "\n",
        "  Create a function that takes a song name as input and outputs a list of songs recommended based on textual similarity. For this, you'll use the cosine similarity scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the liberarys\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import scipy.sparse as ss\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh6JlOR2YX9E"
      },
      "outputs": [],
      "source": [
        "# Refine the Textual Data: Consider merging multiple textual columns (e.g., artist name + track name) to generate recommendations based on combined textual data.\n",
        "\n",
        "# Merge 'artists' and 'track_name' columns with a separator\n",
        "df['song_artist'] = df['artists'] + ' - ' + df['track_name']\n",
        "\n",
        "# Display the DataFrame with the new 'combined_text' column\n",
        "df[['artists', 'track_name', 'song_artist']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ones = np.ones(len(df), np.uint64)\n",
        "matrix = ss.coo_matrix((ones, (df['track_id_id'], df['popularity'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Miscellaneous operation.\n",
        "matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(matrix.row) # check row indices\n",
        "print(matrix.col) # check column indices\n",
        "print(matrix.shape)\n",
        "print(matrix.data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
        "matrix_track_id_id = svd.fit_transform(matrix)\n",
        "matrix_popularity = svd.fit_transform(matrix.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cosine_distance_matrix_places = cosine_distances(matrix_popularity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO0YDiSzYaae"
      },
      "outputs": [],
      "source": [
        "# Filtering by Additional Features: How might you modify the recommender to suggest only songs from a particular genre or only non-explicit songs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzjsZ4L9Ye9B"
      },
      "outputs": [],
      "source": [
        "#  Improving Efficiency: If you have a very large dataset, computing cosine similarities can be time-consuming. How might you address this efficiency concern?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
